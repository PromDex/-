{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ac4b0c",
   "metadata": {},
   "source": [
    "#### ШАБЛОН"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360d1a3b",
   "metadata": {},
   "source": [
    "('customer_id','customer_unique_id','customer_zip_code_prefix','customer_city','customer_state','geolocation_zip_code_prefix','geolocation_lat','geolocation_lng','geolocation_city','geolocation_state','order_id','order_status','order_purchase_timestamp','order_approved_at','order_delivered_carrier_date','order_delivered_customer_date','order_estimated_delivery_date','order_item_id','product_id','seller_id','shipping_limit_date','price','freight_value','payment_sequential','payment_type','payment_installments','payment_value','review_id','review_score','review_comment_title','review_comment_message','review_creation_date','review_answer_timestamp','product_category_name','product_name_lenght','product_description_lenght','product_photos_qty','product_weight_g','product_length_cm','product_height_cm','product_width_cm','seller_zip_code_prefix','seller_city','seller_state','product_category_name_english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a48d4",
   "metadata": {},
   "source": [
    "### Описательная статистика на Python (Центральные метрики и метрики оценки вариативности)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae250698",
   "metadata": {},
   "source": [
    "Описательная статистика — это описание и интегральные параметры наборов данных.\\\n",
    "Если говорить о метриках, то в этой части была проработана центральная метрика (которая говорит нам о центрах концентрации данных, таких как среднее, медиана и мода) и метрика вариативности данных (которая говорит о разбросе значений, таких как дисперсия и стандартное отклонение)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0027ccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "zf = zipfile.ZipFile('brazilian-ecommerce.zip') \n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Create a ZipFile Object and load sample.zip in it\n",
    "with ZipFile('brazilian-ecommerce.zip', 'r') as zipObj:\n",
    "    # Extract all the contents of zip file in current directory\n",
    "    zipObj.extractall()\n",
    "! ls\n",
    "Untitled.ipynb                        olist_order_reviews_dataset.csv\n",
    "brazilian-ecommerce                   olist_orders_dataset.csv\n",
    "brazilian-ecommerce.zip               olist_products_dataset.csv\n",
    "olist_customers_dataset.csv           olist_sellers_dataset.csv\n",
    "olist_geolocation_dataset.csv         product_category_name_translation.csv\n",
    "olist_order_items_dataset.csv         temp_csv\n",
    "olist_order_payments_dataset.csv\n",
    "\n",
    " \n",
    "df_items = pd.read_csv('olist_order_items_dataset.csv')\n",
    "df_orders = pd.read_csv('olist_orders_dataset.csv')\n",
    "df_products = pd.read_csv('olist_products_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7212a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Видел у вас мак - это для вас . У меня в другой директории.\n",
    "import zipfile\n",
    "\n",
    "zf = zipfile.ZipFile('brazilian-ecommerce.zip') \n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Create a ZipFile Object and load sample.zip in it\n",
    "with ZipFile('brazilian-ecommerce.zip', 'r') as zipObj:\n",
    "    # Extract all the contents of zip file in current directory\n",
    "    zipObj.extractall()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bc922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_path = \"https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce/download?datasetVersionNumber=2\"\n",
    "# df = pd.read_csv(\"https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce/download?datasetVersionNumber=2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb7866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile('archive.zip.', 'r') as zipObj:\n",
    "    zipObj.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9204ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "082a8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import statistics\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbecc7dd",
   "metadata": {},
   "source": [
    "После этого, я создал некоторые исходные данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfbcd249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вывод исходных данных, которые содержатся в x:[10.0, 2, 2.5, 5, 26.0]\n",
      "Вывод исходных данных, которые содержатся в x_with_nan:[10.0, 2, 2.5, nan, 5, 26.0]\n"
     ]
    }
   ],
   "source": [
    "x = [10.0, 2, 2.5, 5, 26.0]\n",
    "x_with_nan = [10.0, 2, 2.5, math.nan, 5, 26.0]\n",
    "print(f'Вывод исходных данных, которые содержатся в x:{x}')\n",
    "print(f'Вывод исходных данных, которые содержатся в x_with_nan:{x_with_nan}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71851ae2",
   "metadata": {},
   "source": [
    "Теперь у нас есть списки x и x_with_nan. Они почти одинаковы, с той разницей, что x_with_nan содержат nan значение. Важно понимать поведение процедур статистики Python, когда они сталкиваются с нечисловым значением (nan). В науке о данных пропущенные значения являются общими, и вы часто будете заменять их на nan. Теперь создаем объекты np.ndarray и pd.Series, соответствующие x и x_with_nan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e48b983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вывод данных, которые содержатся в y и y_with_nan:[10.   2.   2.5  5.  26. ], [10.   2.   2.5  nan  5.  26. ]\n",
      "Вывод данных, которые содержатся в z и в z_with_nan: 0    10.0\n",
      "1     2.0\n",
      "2     2.5\n",
      "3     5.0\n",
      "4    26.0\n",
      "dtype: float64, 0    10.0\n",
      "1     2.0\n",
      "2     2.5\n",
      "3     NaN\n",
      "4     5.0\n",
      "5    26.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "y, y_with_nan = np.array(x), np.array(x_with_nan)\n",
    "z, z_with_nan = pd.Series(x), pd.Series(x_with_nan)\n",
    "print(f'Вывод данных, которые содержатся в y и y_with_nan:{y}, {y_with_nan}')\n",
    "print(f'Вывод данных, которые содержатся в z и в z_with_nan: {z}, {z_with_nan}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3422899f",
   "metadata": {},
   "source": [
    "Теперь у нас есть два массива NumPy (y и y_with_nan) и два объекта Series Pandas (z и z_with_nan). Все это — 1D последовательности значений. После формирования исходных данных, приступаем к расчету центральной метрики, а именно среднего значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9968e479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет среднего значения, используя sum и len: 9.1\n",
      "Расчет среднего значения, используя встроенные функции статистики Python (statistics.mean(x)): 9.1\n",
      "Расчет среднего значения, используя встроенные функции статистики Python (statistics.fmean(x)): 9.1\n",
      "Расчет среднего значения, который содержит значения nan, используя встроенные функции статистики Python (statistics.mean(x)): nan\n",
      "Расчет среднего значения, используя NumPy: 9.1\n",
      "Расчет среднего значения с помощью NumPy, игнорируя nan: 9.1\n",
      "Расчет среднего значения объекта pd.Series: 9.1\n"
     ]
    }
   ],
   "source": [
    "#Рассчет средних значений\n",
    "mean_1 = sum(x) / len(x)\n",
    "print(f'Расчет среднего значения, используя sum и len: {mean_1}')\n",
    "mean_2 = statistics.mean(x)\n",
    "print(f'Расчет среднего значения, используя встроенные функции статистики Python (statistics.mean(x)): {mean_2}')\n",
    "mean_3 = statistics.fmean(x)\n",
    "print(f'Расчет среднего значения, используя встроенные функции статистики Python (statistics.fmean(x)): {mean_3}')\n",
    "mean_4 = statistics.mean(x_with_nan)\n",
    "print(f'Расчет среднего значения, который содержит значения nan, используя встроенные функции статистики Python (statistics.mean(x)): {mean_4}')\n",
    "mean_5 = np.mean(y)\n",
    "print(f'Расчет среднего значения, используя NumPy: {mean_5}')\n",
    "np.nanmean(y_with_nan)\n",
    "print(f'Расчет среднего значения с помощью NumPy, игнорируя nan: {np.nanmean(y_with_nan)}')\n",
    "mean_6 = z.mean()\n",
    "print(f'Расчет среднего значения объекта pd.Series: {mean_6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2cc4e9",
   "metadata": {},
   "source": [
    "Первое среднее значение было рассчитано на чистом Python, используя sum() и len(), без импорта библиотек. Хотя это чисто и элегантно, но для расчета второго, третьего и четвертого значения были применены встроенные функции библиотеки statistics Python. При расчете пятого среднего значения была использована библиотека NumPy и функция np.mean. А шестое среднее значение было рассчитано с помощью метода .mean() библиотеки Pandas.\n",
    "\n",
    "Далее, было рассчитано средневзвешенное значение. Средневзвешенное или также называемое средневзвешенным арифметическим или средневзвешенным значением, является обобщением среднего арифметического, которое позволяет вам определить относительный вклад каждой точки данных в результат.\n",
    "\n",
    "Сам расчет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6a839ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет средневзвешанного с помощью range: 6.8\n",
      "Расчет средневзвешанного с помощью zip: 6.8\n",
      "Расчет средневзвешанного с помощью np.average для массивово NumPy или серии Pandas: 6.8\n",
      "Расчет средневзвешанного с помощью поэлементного умножения w * y: 6.8\n",
      "Расчет средневзвешанного для набора, который содержит nan : nan\n"
     ]
    }
   ],
   "source": [
    "#Рассчет средневзвешанных значений\n",
    "x = [6.0, 1, 2.5, 6, 25.0]\n",
    "w = [0.1, 0.2, 0.3, 0.25, 0.15]\n",
    "wmean = sum(w[i] * x[i] for i in range(len(x))) / sum(w)\n",
    "print(f'Расчет средневзвешанного с помощью range: {wmean}')\n",
    "wmean2 = sum(x_ * w_ for (x_, w_) in zip(x, w)) / sum(w)\n",
    "print(f'Расчет средневзвешанного с помощью zip: {wmean2}')\n",
    "y, z, w = np.array(x), pd.Series(x), np.array(w)\n",
    "wmean3= np.average(y, weights=w)\n",
    "print(f'Расчет средневзвешанного с помощью np.average для массивово NumPy или серии Pandas: {wmean3}')\n",
    "o = (w * y).sum() / w.sum()\n",
    "print(f'Расчет средневзвешанного с помощью поэлементного умножения w * y: {o}')\n",
    "w = np.array([0.1, 0.2, 0.3, 0.0, 0.2, 0.1])\n",
    "print(f'Расчет средневзвешанного для набора, который содержит nan : {(w * y_with_nan).sum() / w.sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc77768",
   "metadata": {},
   "source": [
    "Первое и второе средневзвешенное было рассчитано В чистом Python используя комбинацию sum() с range() и zip(). Опять же, это чистая и элегантная реализация, в которой вам не нужно импортировать какие-либо библиотеки. Однако, если у вас большие наборы данных, то NumPy, вероятно, будет лучшим решением. Можно использовать np.average(), как это сделано при расчете третьего показателя, для массивов NumPy или серии Pandas. Для расчета четвертого и пятого показателя, было использовано поэлементное умножение с методом .sum().\n",
    "\n",
    "После этого, было рассчитано гармоническое среднее, что есть обратная величина от среднего значения обратных величин всех элементов в наборе данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7a92418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет гармонического среднего: 2.8195488721804507\n",
      "Расчет гармонического среднего с помощью statistics.harmonic_mean(): 2.819548872180451\n",
      "Расчет гармонического среднего, где есть nan: nan\n",
      "Расчет гармонического среднего, где есть 0: 0\n",
      "Расчет гармонического среднего с помощью  scipy.stats.hmean(): 2.8195488721804507\n"
     ]
    }
   ],
   "source": [
    "#Гармоническое среднее\n",
    "hmean = len(x) / sum(1 / item for item in x)\n",
    "print(f'Расчет гармонического среднего: {hmean}')\n",
    "hmean2 = statistics.harmonic_mean(x)\n",
    "print(f'Расчет гармонического среднего с помощью statistics.harmonic_mean(): {hmean2}')\n",
    "statistics.harmonic_mean(x_with_nan)\n",
    "print(f'Расчет гармонического среднего, где есть nan: {statistics.harmonic_mean(x_with_nan)}')\n",
    "statistics.harmonic_mean([1, 0, 2])\n",
    "print(f'Расчет гармонического среднего, где есть 0: {statistics.harmonic_mean([1, 0, 2])}')\n",
    "scipy.stats.hmean(y)\n",
    "print(f'Расчет гармонического среднего с помощью  scipy.stats.hmean(): {scipy.stats.hmean(y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f2c93",
   "metadata": {},
   "source": [
    "Как обычно, первое значение было рассчитано на чистом Python. Второе, третье и четвертое значение было рассчитано с помощью функции statistics.harmonic_mean(). И последнее было рассчитано используя scipy.stats.hmean.\n",
    "\n",
    "Следом был рассчитан среднее геометрическое:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0c93247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вычисление геометрического среднего: 2.0613255209172676\n",
      "Вычисление геометрического среднего с помощью  statistics.geometric_mean(): 4.682054920046206\n",
      "Вычисление геометрического среднего где есть nan: nan\n",
      "Вычисление геометрического среднего с помощью scipy.stats.gmean(): 4.682054920046206\n"
     ]
    }
   ],
   "source": [
    "#Среднее геометрическое\n",
    "gmean = 1\n",
    "for item in x:\n",
    "    gmean *= item\n",
    "    gmean **= 1 / len(x)\n",
    "print(f'Вычисление геометрического среднего: {gmean}')\n",
    "gmean2 = statistics.geometric_mean(x)\n",
    "print(f'Вычисление геометрического среднего с помощью  statistics.geometric_mean(): {gmean2}')\n",
    "gmean3 = statistics.geometric_mean(x_with_nan)\n",
    "print(f'Вычисление геометрического среднего где есть nan: {gmean3}')\n",
    "scipy.stats.gmean(y)\n",
    "print(f'Вычисление геометрического среднего с помощью scipy.stats.gmean(): {scipy.stats.gmean(y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62f185a",
   "metadata": {},
   "source": [
    "Первое значение было рассчитано на чистом Python. Второе и третье значение было рассчитано с помощью функции statistics.geometric_mean(). И последнее было рассчитано используя scipy.stats.gmean.\n",
    "\n",
    "Медиана — это средний элемент отсортированного набора данных.\n",
    "\n",
    "Расчет медианы представлен внизу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c602c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет медианы: 6.0\n",
      "Расчет медианы с помощью statistics.median(): 6.0\n",
      "Расчет медианы с помощью statistics.median_low: 2.5\n",
      "Расчет медианы с помощью statistics.median_high 6.0\n",
      "Расчет медианы с помощью np.median: 6.0\n"
     ]
    }
   ],
   "source": [
    "n = len(x)\n",
    "if n % 2:\n",
    "    median_ = sorted(x)[round(0.5*(n-1))]\n",
    "else:\n",
    "    x_ord, index = sorted(x), round(0.5 * n)\n",
    "    median_ = 0.5 * (x_ord[index-1] + x_ord[index])\n",
    "print(f'Расчет медианы: {median_}')\n",
    "median_2 = statistics.median(x)\n",
    "print(f'Расчет медианы с помощью statistics.median(): {median_2}')\n",
    "statistics.median_low(x[:-1])\n",
    "print(f'Расчет медианы с помощью statistics.median_low: {statistics.median_low(x[:-1])}')\n",
    "statistics.median_high(x[:-1])\n",
    "print(f'Расчет медианы с помощью statistics.median_high {statistics.median_high(x[:-1])}')\n",
    "median_2 = np.median(y)\n",
    "print(f'Расчет медианы с помощью np.median: {median_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b33ea3",
   "metadata": {},
   "source": [
    "Первое значение было рассчитано на чистом Python. Следующие три были найдены используя statistics.median, при этом, median_low() возвращает меньшее, а median_high() — большее среднее значение. И последняя была найдена с помощью NumPy и функции np.median().\n",
    "\n",
    "Мода — это значение в наборе данных, которое встречается чаще всего. Если такого значения не существует, набор является мультимодальным, поскольку он имеет несколько модальных значений. Расчет моды представлен внизу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73df3a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вычисление моды: 2\n",
      "Вычисление моды с помощью statistics.mode(): 2\n",
      "Вычисление моды с помощью statistics.multimode(): [2]\n",
      "Вычисление моды с помощью scipy.stats.mode(): ModeResult(mode=array([2]), count=array([2]))\n"
     ]
    }
   ],
   "source": [
    "u = [2, 3, 2, 8, 12]\n",
    "mode_ = max((u.count(item), item) for item in set(u))[1]\n",
    "print(f'Вычисление моды: {mode_}')\n",
    "mode_2 = statistics.mode(u)\n",
    "print(f'Вычисление моды с помощью statistics.mode(): {mode_2}')\n",
    "mode_3 = statistics.multimode(u)\n",
    "print(f'Вычисление моды с помощью statistics.multimode(): {mode_3}')\n",
    "mode_4 = scipy.stats.mode(u)\n",
    "print(f'Вычисление моды с помощью scipy.stats.mode(): {mode_4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7f8da0",
   "metadata": {},
   "source": [
    "Первое значение, как обычно, получено используя чистый Python. Вы используете u.count(), чтобы получить количество вхождений каждого элемента в u. Элемент с максимальным количеством вхождений — это мода. Обратите внимание, что вам не нужно использовать set(u). Вместо этого вы можете заменить его просто на u и повторить весь список. Второе и третье значение было вычислено с помощью statistics.mode() и statistics.multimode(). Обратите внимание, mode() вернула одно значение, а multimode() в результате вернула список. Однако, это не единственное различие между двумя функциями. Если существует более одного модального значения, то mode() вызывает StatisticsError, а multimode() возвращает список со всеми режимами. И последнее значение было найдено с помощью функции, которая возвращает объект с модальным значением и количество его повторений в наборе данных.\n",
    "\n",
    "Центральных метрик недостаточно для описания данных. Практически всегда необходимы метрики оценки вариативности данных, которые количественно определяют разброс точек данных. И первым показателем метрики оценки вариативности данных была дисперсия. Дисперсия количественно определяет разброс данных. Численно показывает, как далеко точки данных от среднего значения.\n",
    "\n",
    "Сам расчет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e176f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оценка дисперсии на чистом Python: 94.04999999999998\n",
      "Оценка дисперсии с помощью statistics.variance(): 94.04999999999998\n",
      "Оценка дисперсии с помощью statistics.variance(), где есть nan: nan\n",
      "Оценка дисперсии, используя NumPy с помощью np.var(): 94.04999999999998\n",
      "Оценка дисперсии, используя NumPy с помощью метода .var(): 94.04999999999998\n"
     ]
    }
   ],
   "source": [
    "n = len(x)\n",
    "mean = sum(x) / n\n",
    "var_ = sum((item - mean)**2 for item in x) / (n - 1)\n",
    "print(f'Оценка дисперсии на чистом Python: {var_}')\n",
    "var_1= statistics.variance(x)\n",
    "print(f'Оценка дисперсии с помощью statistics.variance(): {var_1}')\n",
    "statistics.variance(x_with_nan)\n",
    "print(f'Оценка дисперсии с помощью statistics.variance(), где есть nan: {statistics.variance(x_with_nan)}')\n",
    "var_2 = np.var(y, ddof=1)\n",
    "print(f'Оценка дисперсии, используя NumPy с помощью np.var(): {var_2}')\n",
    "var_3 = y.var(ddof=1)\n",
    "print(f'Оценка дисперсии, используя NumPy с помощью метода .var(): {var_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ba7953",
   "metadata": {},
   "source": [
    "Первый метод расчета – используя чистый Python. В целом, этого достаточно и можно правильно дать оценку дисперсии. Однако, более короткое и элегантное решение — использовать функцию statistics.variance() (как сделано это при расчете второго показателя). В результате мы получили тот же результат для дисперсии, что и в первом. И оставшиеся последние два показателя были рассчитаны используя NumPy, а именно функции np.var() и метода .var()\n",
    "\n",
    "Далее, было рассчитано среднеквадратичное отклонение. Стандартное отклонение выборки является еще одним показателем разброса данных. Он связан с оценкой дисперсией, поскольку стандартное отклонение есть положительным квадратный корень из оценки дисперсии. Стандартное отклонение часто более удобно, чем дисперсия, потому что имеет ту же размерность, что и данные. Сам расчет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fda9334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет среднеквадратичного отклонения на чистом Python: 9.697937925146768\n",
      "Расчет среднеквадратичного отклонения с помощью  statistics.stdev(): 9.697937925146768\n",
      "Расчет среднеквадратичного отклонения с помощью  NumPy: 9.697937925146768\n"
     ]
    }
   ],
   "source": [
    "#Среднеквадратичное отклонение\n",
    "std_ = var_ ** 0.5\n",
    "print(f'Расчет среднеквадратичного отклонения на чистом Python: {std_}')\n",
    "std_2 = statistics.stdev(x)\n",
    "print(f'Расчет среднеквадратичного отклонения с помощью  statistics.stdev(): {std_2}')\n",
    "np.std(y, ddof=1)\n",
    "print(f'Расчет среднеквадратичного отклонения с помощью  NumPy: {np.std(y, ddof=1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88bd709",
   "metadata": {},
   "source": [
    "После этого, было найдено смещение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8d766c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет смещения на чистом Python: 1.9470432273905929\n",
      "Расчет смещения с помощью Pandas: 1.9470432273905924\n"
     ]
    }
   ],
   "source": [
    "#Смещение\n",
    "x = [8.0, 1, 2.5, 4, 28.0]\n",
    "n = len(x)\n",
    "mean_ = sum(x) / n\n",
    "var_ = sum((item - mean_)**2 for item in x) / (n - 1)\n",
    "std_ = var_ ** 0.5\n",
    "skew_ = (sum((item - mean_)**3 for item in x)\n",
    "       * n / ((n - 1) * (n - 2) * std_**3))\n",
    "print(f'Расчет смещения на чистом Python: {skew_}')\n",
    "z, z_with_nan = pd.Series(x), pd.Series(x_with_nan)\n",
    "print(f'Расчет смещения с помощью Pandas: {z.skew()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b89345",
   "metadata": {},
   "source": [
    "Первый показатель, был найден, соответственно с помощью чистого Python, а второй с помощью Pandas, используя метод .skew().\n",
    "\n",
    "Процентиль — такой элемент в наборе данных, так что p элементов в наборе данных меньше или равно его значению. Кроме того, (100 - p) элементов больше или равно этому значению. Если в наборе данных есть два таких элемента, то процентиль является их средним арифметическим. Расчет процентиля представлен внизу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b49059c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расчет процентилей с помощью  statistics.quantiles(): [8.0]\n",
      "Расчет процентилей с помощью  statistics.quantiles(): [0.1, 8.0, 21.0]\n",
      "Нахождение 5 процентиля : -3.44\n",
      "Нахождение 95 процентиля : 34.919999999999995\n",
      "Нахождение процентиля используя метод .quantile(): -3.44\n"
     ]
    }
   ],
   "source": [
    "#Процентили\n",
    "x = [-5.0, -1.1, 0.1, 2.0, 8.0, 12.8, 21.0, 25.8, 41.0]\n",
    "print(f'Расчет процентилей с помощью  statistics.quantiles(): {statistics.quantiles(x, n=2)}')\n",
    "statistics.quantiles(x, n=4, method='inclusive')\n",
    "print(f\"Расчет процентилей с помощью  statistics.quantiles(): {statistics.quantiles(x, n=4, method='inclusive')}\")\n",
    "y = np.array(x)\n",
    "np.percentile(y, 5)\n",
    "print(f'Нахождение 5 процентиля : {np.percentile(y, 5)}')\n",
    "np.percentile(y, 95)\n",
    "print(f'Нахождение 95 процентиля : {np.percentile(y, 95)}')\n",
    "z, z_with_nan = pd.Series(y), pd.Series(y_with_nan)\n",
    "z.quantile(0.05)\n",
    "print(f'Нахождение процентиля используя метод .quantile(): {z.quantile(0.05)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fb52f2",
   "metadata": {},
   "source": [
    "Первый показатель был найден с помощью statistics.quantiles.В этом примере 8,0 — медиана x, а 0,1 и 21,0 — это 25-й и 75-й процентили выборки соответственно. Параметр n определяет количество результирующих процентилей с равной вероятностью, а метод определяет, как их вычислять. Следующие показатели, а именно 5 и 95 процентили, были найдены с помощью библиотеки NumPy, функции np.percentile(). И последний показатель был найдет используя метод .quantile().\n",
    "\n",
    "Диапазон данных — это разница между максимальным и минимальным элементом в наборе данных. Эти показатели я нашел используя функцию np.ptp():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aedb104a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нахождение диапазона с помощью функции  np.ptp(): (46.0, 46.0, nan, nan)\n"
     ]
    }
   ],
   "source": [
    "#Диапазон\n",
    "np.ptp(y)\n",
    "np.ptp(z)\n",
    "np.ptp(y_with_nan)\n",
    "np.ptp(z_with_nan)\n",
    "print(f'Нахождение диапазона с помощью функции  np.ptp(): {np.ptp(y),np.ptp(z),np.ptp(y_with_nan),np.ptp(z_with_nan)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7566b41",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8516bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сводка описательной статистики с помощью  scipy.stats.describe(): DescribeResult(nobs=9, minmax=(-5.0, 41.0), mean=11.622222222222222, variance=228.75194444444446, skewness=0.9249043136685094, kurtosis=0.14770623629658886)\n",
      "Сводка описательной статистики с помощью  метода .describe() в Pandas: count     9.000000\n",
      "mean     11.622222\n",
      "std      15.124548\n",
      "min      -5.000000\n",
      "25%       0.100000\n",
      "50%       8.000000\n",
      "75%      21.000000\n",
      "max      41.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Сводка описательной статистики\n",
    "result = scipy.stats.describe(y, ddof=1, bias=False)\n",
    "print(f'Сводка описательной статистики с помощью  scipy.stats.describe(): {result}')\n",
    "result2 = z.describe()\n",
    "print(f'Сводка описательной статистики с помощью  метода .describe() в Pandas: {result2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577758a3",
   "metadata": {},
   "source": [
    "Первый показатель был найден с помощью scipy.stats.describe(). В качестве первого аргумента необходимо передать набор данных, который может быть представлен массивом NumPy, списком, кортежем или любой другой подобной структурой данных. Можно опустить ddof = 1, так как это значение по умолчанию и имеет значение только при расчете дисперсии. Указано bias = False для принудительного исправления асимметрии и эксцесса статистического смещения.\n",
    "description() возвращает объект, который содержит следующую описательную статистику:\n",
    "• nobs — количество наблюдений или элементов в вашем наборе данных;\n",
    "• minmax — кортеж с минимальными и максимальными значениями;\n",
    "• mean — среднее значение;\n",
    "• variance — дисперсия;\n",
    "• skewness — асимметрия;\n",
    "• kurtosis — эксцесс вашего набора данных.\n",
    "Второй показатель был найден с помощью метода .describe() библиотеки Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e35bc76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed9bc729",
   "metadata": {},
   "source": [
    "- ОБРАБОТКА"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a1d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://www.dropbox.com/s/62xm9ymoaunnfg6/bank-full.csv?dl=1', sep=';')\n",
    "df.head()\n",
    "# Выведем первые 10 строк\n",
    "# Наш фрейм данных называется data, поэтому мы вызываем data.head(10) и\n",
    "# печатаем результат\n",
    "print(data.head(10))\n",
    "\n",
    "#Оценка столбцов/поиск недостающих значений Как видим, в прочитанном наборе всего 8 столбцов.\n",
    "#Чтобы получить более подробное представление, давайте воспользуемся функцией df.info и узнаем больше о столбцах,\n",
    "#с которыми мы имеем дело:\n",
    "data.info()\n",
    "\n",
    "Удаление дубликатов\n",
    "Дублирующие записи не только искажают статистические показатели датасета, но и снижают качество обучения модели, потому удалим полные дублирующие вхождения. Для начала уточним, сколько записей в датасете с помощью свойства Pandas.DataFrame.shape:\n",
    ">>> df.shape\n",
    "(41188, 21)\n",
    "# Удалим дублирующие записи с помощью Pandas.drop_duplicates() и обновим данные о размере данных:\n",
    ">>> df = df.drop_duplicates()\n",
    ">>> df.shape\n",
    "(41176, 21)\n",
    "# Стоит помнить, что в случае, если пропусков у признака слишком много (более 70%), такой признак удаляют.\\\n",
    "# Проверим, насколько полны наши признаки: метод isnull() пройдется по каждой ячейке каждого столбца и определит,\\\n",
    "# кто пуст, а кто нет, составив датафрейм такого же размера, состоящий из True / False.\\\n",
    "# Метод mean() суммирует все значения True, определит концентрацию пропусков в каждом столбце.\\\n",
    "# На 100 мы умножаем, чтобы получить значение в процентах:\n",
    "df.isnull().mean() * 100\n",
    "# Таким образом, переменная подлежит удалению с помощью drop():\n",
    "df = df.drop(columns=['День'])\n",
    "# Встроенные методы Pandas позволяют с легкостью справиться с первыми двумя разновидностями таких пробелов.\\\n",
    "# Разберемся для начала с категориальными переменными, объединив их в один вектор.\\\n",
    "# Список получится совсем уж нелогичный, но это не столь важно в данной ситуации: мы лишь ищем способы обозначения пропуска.\n",
    "\n",
    ">>> column_values = df[['Работа', 'Семейный статус', 'Образование', 'Контакт', 'Месяц', 'День недели', 'Доходность']].values.ravel()\n",
    ">>> unique_values =  pd.unique(column_values)\n",
    ">>> print(unique_values)\n",
    "\n",
    "['Самозанятый' 'Не женат / не замужем' 'Университетская степень'\n",
    " 'Городской телефон' 'Октябрь' 'Пятница' 'Отсутствует' 'Преддприниматель'\n",
    " 'Женат / замужем' 'Голубой воротничок' 'Базовое (9 классов)' 'Менеджер'\n",
    " 'Высшая школа' 'Базовое (4 класса)' 'Техник' 'Профессиональный курс'\n",
    " 'Разведен(-а)' 'Неизвестно' 'Сотовый телефон' 'Август' 'Понедельник'\n",
    " 'Студент' 'Домохозяйка' 'Обслуживающий персонал' 'Базовое (6 классов)'\n",
    " 'Пенсионер' 'Четверг' 'Вторник' 'Не присутствует' 'Июль' 'Среда' 'Июнь'\n",
    " 'Неграмотный' 'Май' 'Ноябрь' 'Присутствует' 'Cамозанятый' 'Декабрь'\n",
    " 'Март' 'Апрель' 'Сентябрь']\n",
    "\n",
    "# Из общего списка уникальных значений этих переменных пропуски обозначаются словом \"Неизвестно\".\\\n",
    "# Для числовых переменных пропуски – число 999 или пустая ячейка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f983af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Одномерный анализ\n",
    "\n",
    "#Описательная статистика\n",
    "#Прежде чем применять те или иные методы обучения, нам необходимо удостовериться, что они применимы к текущему датасету.\n",
    "#Раздел описательной статистики включает в себя проверку на нормальность распределения и определение прочих статистических метрик.\n",
    "#С этим нам поможет замечательная библиотека pandas-profiling. Установим самую свежую версию во избежание ошибок:\n",
    "\n",
    "!pip install pandas_profiling --upgrade\n",
    "#Запустим профайлер и передадим df в качестве аргумента:\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(df)\n",
    "profile\n",
    "\n",
    "\n",
    "# Профайлер высчитывает основные статистические метрики для каждой переменной и датасета в целом:\n",
    "#К примеру, в признаке \"Длительность\" мы вычислили:\n",
    "\n",
    "#Количество уникальных значений (Distinct)\n",
    "#Количество пропусков (Missing)\n",
    "#Вероятно, параметр \"Бесконечность\" ('Infinite'), рассчитываемый только для вещественных чисел, отыскивает сильно выделяющиеся значения, которыми иногда обозначают пропуски.\n",
    "#Среднее значение (Mean)\n",
    "#Минимум (Minimum)\n",
    "#Максимум (Maximum)\n",
    "#Количество нулей (Zeros)\n",
    "#Память, задействованная этой переменной (Memory Size)\n",
    "#Нормальность распределения (график)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eea77e1",
   "metadata": {},
   "source": [
    "Следующий интересный раздел – \"Корреляции\" ('Correlations').\n",
    "Чем ярче (краснее / синее) ячейка, тем сильнее выражена корреляция между парой признаков.\n",
    "Диагональные ячейки игнорируются, поскольку являются результатом расчета коэффициента между переменной и ее копией.\n",
    "\n",
    "Профайлер вычленил из датасета только числовые признаки, и потому матрица имеет размер 11 x 11. К примеру, \"колебание уровня безработицы\" и \"европейская межбанковская ставка\" сильно коррелируют друг с другом, но поскольку эти признаки второстепенны, в дальнейшем их можно объединить на этапе инжиниринга признаков (Feature Engineering). Зачастую целевая переменная не сильно коррелирует с предикторами.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104dabe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Важность признаков\n",
    "# Прежде чем произвести инжиниринг признаков и сократить объем входных данных, стоит определить,\n",
    "# какие признаки имеют первостепенную значимость, и в этом нам поможет Scikit-Learn и критерий Хи-квадрат (Chi-Squared Test).:\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "X = df[['Возраст', 'Длительность', 'Кампания', 'День', 'Предыдущий контакт', 'Индекс потребительских цен', 'Европейская межбанковская ставка', 'Количество сотрудников в компании']]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "bestfeatures = SelectKBest(score_func = chi2, k = 'all')\n",
    "fit = bestfeatures.fit(X, y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "featureScores = pd.concat([dfcolumns, dfscores], axis = 1)\n",
    "featureScores.columns = ['Specs', 'Score']  \n",
    "print(featureScores.nlargest(10, 'Score'))  \n",
    "\n",
    "\n",
    "# Неожиданно, но самым важным признаком оказалась длительность разговора и день звонка.\n",
    "# Люди склонны брать кредитные продукты, если им позвонили в определенный день недели и разговор длился оптимальное время.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3853d",
   "metadata": {},
   "source": [
    "Многомерный анализ\n",
    "\n",
    "Рассмотрение парных особенностей\n",
    "Чего только не создаст комьюнити в Науке о данных! Для нужд разведочного анализа крайне кстати будет и попарные графики, и здесь на помощь приходит другой великолепный класс - seaborn.pairplot().\n",
    "\n",
    "Каждая из переменных ляжет в основу одной из осей двумерного точечного графика, и так, пока все пары признаков не будут отображены. Сократим названия длинных переменных, чтобы уместить их на скромном отведенном пространстве:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ba7245",
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.pairplot()\n",
    "\n",
    "#Анализ главных компонент (Principal Component Analysis) представляет собой метод уменьшения размерности больших наборов данных\n",
    "#путем преобразования большого набора переменных в меньший с минимальными потерями информативности.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
    "principalDf.head()\n",
    "\n",
    "# Мы получили два принципиальных компонента и путем такого сокращения понижаем размерность датасета без потерь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f24481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Нормализация\n",
    "#Еще один шаг, не затронутый в примере выше, – это Нормализация (Normalization), и порой приходится выбирать между ею и стандартизацией. Мы нормализуем те же признаки, характеризующие состояние экономики и потому загрузим датасет в исходном виде еще раз:\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df = pd.read_csv('https://www.dropbox.com/s/62xm9ymoaunnfg6/bank-full.csv?dl=1', sep=';')\n",
    "\n",
    "# Выберем признаки, выраженные вещественными числами и подлежащие \n",
    "# нормализации\n",
    "features = ['Колебание уровня безработицы', 'Индекс потребительских цен', 'Индекс потребительской уверенности', 'Европейская межбанковская ставка']\n",
    "x = df.loc[:, features].values\n",
    "\n",
    "# Инициализируем нормализатор \n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df = pd.DataFrame(x_scaled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096e0637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Уменьшение размерности, стандартизация\n",
    "#Рассмотрев признаки по отдельности и попарно, мы пришли к выводу, что некоторые признаки могут быть как бы объединены\n",
    "#с помощью специальной техники – Анализ главных компонент (PCA).\n",
    "#Итак, давайте создадим заменяющий столбец, который представляет эти признаки в равной мере и тем самым уменьшим размер данных.\n",
    "\n",
    "# Создадим список признаков, подлежащих уменьшению\n",
    "features = ['Колебание уровня безработицы', 'Индекс потребительских цен', 'Индекс потребительской уверенности', 'Европейская межбанковская ставка']\n",
    "\n",
    "# Выбираем сокращаемые признаки и целевой\n",
    "x = df.loc[:, features].values\n",
    "y = df.loc[:,['y']].values\n",
    "#Выполняем Стандартизацию (Standartization) x, и это впоследствии станет частью тренировочных данных:\n",
    "\n",
    ">>> from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x = StandardScaler().fit_transform(x)\n",
    "pd.DataFrame(data = x, columns = features).head()\n",
    "\n",
    "StandardScaler()\n",
    "#на месте заменяет данные на их стандартизированную версию, и мы получаем признаки,\n",
    "#где все значения как бы центрованы относительно нуля.\n",
    "#Такое преобразование необходимо, чтобы сократить нагрузку на вычислительную систему компьютера, который будет обучать модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5f691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Процесс обработки пропусков, к счастью, можно сократить с помощью sklearn.impute.SimpleImputer.\n",
    "# Мы выбираем все категориальные переменные и применяем стратегию \"[вставить вместо пропуска] самое распространенное значение\":\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\n",
    "\n",
    "df[\"Работа\"] = imputer.fit_transform(df[\"Работа\"].values.reshape(-1,1))[:,0]\n",
    "\n",
    "df[\"Семейный статус\"] = imputer.fit_transform(df[\"Семейный статус\"].values.reshape(-1,1))[:,0]\n",
    "\n",
    "df[\"Образование\"] = imputer.fit_transform(df[\"Образование\"].values.reshape(-1,1))[:,0]\n",
    "\n",
    "df[\"Месяц\"] = imputer.fit_transform(df[\"Месяц\"].values.reshape(-1,1))[:,0]\n",
    "\n",
    "df[\"День недели\"] = imputer.fit_transform(df[\"День недели\"].values.reshape(-1,1))[:,0]\n",
    "\n",
    "df[\"Доходность\"] = imputer.fit_transform(df[\"Доходность\"].values.reshape(-1,1))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f28d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Признаки, принадлежащие к булевому типу данных, обрабатываются алгоритмом тем же образом.\n",
    "# Целевую переменную Y мы не обрабатываем (если в этом столбце есть пропуски, такие строки стоит удалить):\n",
    "\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\n",
    "\n",
    "df[\"Кредитный дефолт\"] = imputer.fit_transform(df[\"Кредитный дефолт\"].values.reshape(-1,1))[:,0]\n",
    "\n",
    "df[\"Ипотека\"] = imputer.fit_transform(df[\"Ипотека\"].values.reshape(-1,1))[:,0]\n",
    "\n",
    "df[\"Займ\"] = imputer.fit_transform(df[\"Займ\"].values.reshape(-1,1))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Подобным образом заполняются пустоты в числовых переменных, только стратегия теперь – \"вставить среднее значение\".\n",
    "\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "\n",
    "df[\"Возраст\"] = imputer.fit_transform(df[\"Возраст\"].values.reshape(-1,1))[:,0]\n",
    "\n",
    "df[\"Длительность\"] = imputer.fit_transform(df[\"Длительность\"].values.reshape(-1,1))[:,0]\n",
    "\n",
    "df[\"Кампания\"] = imputer.fit_transform(df[\"Кампания\"].values.reshape(-1,1))[:,0]\n",
    "\n",
    "df[\"Предыдущий контакт\"] = imputer.fit_transform(df[\"Предыдущий контакт\"].values.reshape(-1,1))[:,0]\n",
    "\n",
    "df[\"Колебание уровня безработицы\"] = imputer.fit_transform(df[\"Колебание уровня безработицы\"].values.reshape(-1,1))[:,0]\n",
    "\n",
    "df[\"Индекс потребительских цен\"] = imputer.fit_transform(df[\"Индекс потребительских цен\"].values.reshape(-1,1))[:,0]\n",
    "\n",
    "df[\"Индекс потребительской уверенности\"] = imputer.fit_transform(df[\"Индекс потребительской уверенности\"].values.reshape(-1,1))[:,0]\n",
    "\n",
    "df[\"Европейская межбанковская ставка\"] = imputer.fit_transform(df[\"Европейская межбанковская ставка\"].values.reshape(-1,1))[:,0]\n",
    "\n",
    "df[\"Количество сотрудников в компании\"] = imputer.fit_transform(df[\"Количество сотрудников в компании\"].values.reshape(-1,1))[:,0]\n",
    "\n",
    "\n",
    "\n",
    " #Метод isnull() пробегается по каждой ячейке каждого признака и определяет, пустая ли ячейка,\n",
    "# возвращая True / False. Метод mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e89a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Обнаружение аномалий\n",
    "#Самый легкий способ обнаружить выбросы – визуальный.\n",
    "#Мы построим разновидность графика \"ящик с усами\" для одной из числовых переменных –  \"Возраст\":\n",
    "data.isnull().sum()\n",
    "\n",
    "#Мы построим разновидность графика \"ящик с усами\" для одной из числовых переменных –  \"Возраст\":\n",
    "sns.set_style('darkgrid')\n",
    "sns.countplot(y='Пол',data=data,palette='colorblind')\n",
    "plt.xlabel('Количество')\n",
    "plt.ylabel('Пол')\n",
    "plt.show()\n",
    "\n",
    "#График seaborn довольно точно отражает разделение данных на мужчин и женщин. Вот более точная версия подсчета:\n",
    "female_count = len(data[data['Пол']=='Женщина'])\n",
    "male_count = len(data) - female_count\n",
    "print(\"\\n Всего женщин:\",female_count,\"\\n\",\"Всего мужчин:\",male_count)\n",
    "\n",
    "#Теперь давайте посмотрим, как данные делят учащихся на разные расы или этнические группы.\n",
    "#Будем следовать той же процедуре, что и на предыдущем шаге. График можно построить с помощью следующего кода:\n",
    "sns.set_style('whitegrid')\n",
    "sns.countplot(x='Раса/Этнос',data=data,palette='colorblind')\n",
    "plt.xlabel(\"Раса/Этнос\")\n",
    "plt.ylabel(\"Количество\")\n",
    "plt.show() \n",
    "\n",
    "# Затем мы сделаем то же самое, чтобы изучить распределение для 'Уровень образования родителей'.\n",
    "# Посмотрим, что у нас там есть.\n",
    "sns.set_style('whitegrid')\n",
    "sns.countplot(y='Уровень образования родителей',data=data,palette='colorblind')\n",
    "plt.xlabel('Количество')\n",
    "plt.ylabel('Раса/Этнос')\n",
    "plt.show()\n",
    "\n",
    "# Двумерный анализ Далее мы исследуем, есть ли какая-либо корреляция (зависимость) между отдельными функциями (столбцами),\n",
    "# которые нам необходимо учитывать.\n",
    "# Некоторые модели, такие как Наи́вный ба́йесовский классифика́торએ, используют допущение об отсутствии корреляции\n",
    "# между отдельными характеристиками, поэтому этот шаг имеет решающее значение.\n",
    "\n",
    "#Итак, давайте построим диаграммы рассеяния для различных комбинаций предметов.\n",
    "sns.set_style('darkgrid')\n",
    "plt.title('Зависимость между оценками по математике и чтению',size=16)\n",
    "plt.xlabel('Оценка по математике',size=12)\n",
    "plt.ylabel('Оценка по чтению',size=12)\n",
    "sns.scatterplot(x='Оценка по математике', y='Оценка по чтению', data=data, hue='Пол', edgecolor='black', palette='cubehelix', hue_order=['Мужчина','Женщина'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.title('Зависимость между оценками по математике и письму',size=16)\n",
    "plt.xlabel('Оценка по математике',size=12)\n",
    "plt.ylabel('Оценка по письму',size=12)\n",
    "sns.scatterplot(x='Оценка по математике', y='Оценка по письму', data=data, hue='Пол', s=90, edgecolor='black', palette='cubehelix', hue_order=['Мужчина','Женщина'])\n",
    "plt.show()\n",
    "\n",
    "# Итак, диаграмма рассеянияએ предполагает высокую степень корреляции между оценками учащихся по разным предметам.\n",
    "# Оценки учащихся по математике и (чтению, письму) мало разбросаны, но, как правило, они показывают рост, поэтому,\n",
    "# если ученик набирает больше по математике, то он или она также обычно набирает больше по другим предметам.\n",
    "# С другой стороны, зависимость оценок по чтению и письму более сгруппирована вдоль прямой линии.\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.title('Зависимость между оценками по чтению и письму',size=16)\n",
    "plt.xlabel('Оценка по чтению',size=12)\n",
    "plt.ylabel('Оценка по письму',size=12)\n",
    "sns.scatterplot(x='Оценка по чтению', y='Оценка по письму', data=data, hue='Пол', s=90, edgecolor='black', palette='colorblind',hue_order=['Мужчина','Женщина'])\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec9b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Мы знаем, что «интегральная оценка» — это обобщенный показатель, рассчитанный на основе значений измерений\n",
    "# и характеризующий конкретный набор данных. Поэтому, с технической точки зрения, она всегда вызывает особый интерес:\n",
    "# какое из измерений больше всего влияет на её значение.\n",
    "\n",
    "total_marks = ((data['Оценка по математике'] + data['Оценка по чтению'] + data['Оценка по письму'])/300)*100 \n",
    "data['Интегральная оценка'] = total_marks\n",
    "kde_data = data[ ['Оценка по математике','Оценка по чтению','Оценка по письму','Интегральная оценка'] ]\n",
    " \n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.kdeplot(data=kde_data,shade=True, palette='colorblind')\n",
    "plt.show() \n",
    "\n",
    "# Совершенно очевидно, что почти все предметы в одинаковой степени влияют на общий балл. Таким образом, нам не нужно рассматривать какую-либо конкретную функцию, влияющую на Интегральную оценку больше, чем другие.\n",
    "\n",
    "# На сегодня все, ребята! Хотя EDA на этом не заканчивается и есть еще много чего, что надо знать,\n",
    "# но это уже другая история. А теперь пора убедиться, что вы сможете реализовать это самостоятельно для понимания истинной сущности EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352cafef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe46b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сброс ограничений на количество выводимых рядов\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_rows', 7)\n",
    "\n",
    "# Сброс ограничений на число столбцов\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth, 10)\n",
    "\n",
    "# Сброс ограничений на количество символов в записи\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# временно вызвать команду\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    print(df)              \n",
    "              \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2796165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Обнаружение аномалий\n",
    "#Самый легкий способ обнаружить выбросы – визуальный.\n",
    "# Мы построим разновидность графика \"ящик с усами\" для одной из числовых переменных –  \"Возраст\":\n",
    "# Скучковавшиеся окружности в верхней части изображения – и есть аномалии, и от них,\n",
    "# как правило, избавляются с помощью квантилей:\n",
    "\n",
    "q = df[\"Возраст\"].quantile(0.99)\n",
    "q2 = df[\"Длительность\"].quantile(0.99)\n",
    "q3 = df[\"Кампания\"].quantile(0.99)\n",
    "q5 = df[\"Предыдущий контакт\"].quantile(0.99)\n",
    "q6 = df[\"Колебание уровня безработицы\"].quantile(0.99)\n",
    "q7 = df[\"Индекс потребительских цен\"].quantile(0.99)\n",
    "q8 = df[\"Индекс потребительской уверенности\"].quantile(0.99)\n",
    "q9 = df[\"Европейская межбанковская ставка\"].quantile(0.99)\n",
    "q10 = df[\"Количество сотрудников в компании\"].quantile(0.99)\n",
    "\n",
    "df[df[\"Возраст\"] < q]\n",
    "df[df[\"Длительность\"] < q2]\n",
    "df[df[\"Кампания\"] < q3]\n",
    "df[df[\"Предыдущий контакт\"] < q5]\n",
    "df[df[\"Колебание уровня безработицы\"] < q6]\n",
    "df[df[\"Индекс потребительских цен\"] < q7]\n",
    "df[df[\"Индекс потребительской уверенности\"] < q8]\n",
    "df[df[\"Европейская межбанковская ставка\"] < q9]\n",
    "df[df[\"Количество сотрудников в компании\"] < q10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990586db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b21a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f01534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf4cfff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8c406c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
